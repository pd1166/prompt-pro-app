import streamlit as st
import google.generativeai as genai
import time
import pandas as pd
from datetime import datetime

# ==========================================
# 1. ×”×’×“×¨×•×ª ×ª×¦×•×¨×”
# ==========================================
st.set_page_config(
    page_title="Prompt Engineer Pro V12",
    page_icon="ğŸ§ ",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# ==========================================
# 2. ×ª×™×§×•×Ÿ ×¢×™×¦×•×‘ ×•×¦×‘×¢×™× (Colors & Layout Fix)
# ==========================================
st.markdown("""
    <style>
        /* --- ×ª×™×§×•×Ÿ ×¤×¨×™×¡×” (Layout) --- */
        .stApp { direction: ltr; background-color: #FAFAFA; }
        
        /* --- ×ª×™×§×•×Ÿ ×¦×‘×¢×™× ×§×¨×™×˜×™ (Color Fix) --- */
        /* ××›×¨×™×— ××ª ×›×œ ×”×˜×§×¡×˜×™× ×œ×”×™×•×ª ×›×”×™× ×›×“×™ ×©×™×¨××• ××•×ª× ×¢×œ ×”×¨×§×¢ ×”×œ×‘×Ÿ */
        .stApp, .element-container, .stMarkdown, h1, h2, h3, h4, h5, h6, p, div, span {
            color: #212121 !important; /* ×©×—×•×¨ ×›×”×” */
            direction: rtl; 
            text-align: right;
        }
        
        /* --- ×ª×™×§×•×Ÿ ×©×“×•×ª ×§×œ×˜ --- */
        .stTextInput input, .stTextArea textarea { 
            direction: rtl; 
            text-align: right; 
            background-color: #FFFFFF !important; /* ×¨×§×¢ ×œ×‘×Ÿ */
            color: #000000 !important; /* ×˜×§×¡×˜ ×©×—×•×¨ */
            border: 1px solid #E0E0E0;
        }
        
        /* --- ×ª×™×§×•×Ÿ ×ª×¤×¨×™×˜×™× --- */
        .stSelectbox div[data-baseweb="select"] > div { 
            direction: rtl; 
            text-align: right;
            color: #000000 !important;
        }
        
        /* --- ×™×™×©×•×¨ ×¡×¨×’×œ ×¦×“ --- */
        section[data-testid="stSidebar"] > div { 
            direction: rtl; 
            text-align: right; 
            background-color: #F0F2F6;
        }
        section[data-testid="stSidebar"] p, section[data-testid="stSidebar"] span {
            color: #212121 !important;
        }
        
        /* --- ×›×¤×ª×•×¨ ×¨××©×™ --- */
        .stButton button { 
            width: 100%; 
            border-radius: 12px; 
            height: 55px; 
            font-weight: bold; 
            font-size: 18px;
            background: linear-gradient(90deg, #6a11cb 0%, #2575fc 100%); 
            color: white !important; /* ×˜×§×¡×˜ ×œ×‘×Ÿ ×‘×›×¤×ª×•×¨ */
            border: none;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        
        /* ×”×¡×ª×¨×ª ×¨×›×™×‘×™× ××™×•×ª×¨×™× */
        #MainMenu {visibility: hidden;} footer {visibility: hidden;} header {visibility: hidden;}
    </style>
""", unsafe_allow_html=True)

# ==========================================
# 3. × ×™×”×•×œ ×–×™×›×¨×•×Ÿ
# ==========================================
if 'history' not in st.session_state:
    st.session_state.history = []

def add_to_history(original_request, refined_prompt, model_rec, used_model):
    timestamp = datetime.now().strftime("%H:%M")
    st.session_state.history.insert(0, {
        "time": timestamp,
        "original": original_request,
        "prompt": refined_prompt,
        "recommendation": model_rec,
        "engine": used_model
    })

# ==========================================
# 4. ×œ×•×’×™×§×” ×¢×¡×§×™×ª
# ==========================================
CONTEXT_LOGIC = {
    "×©×™×•×•×§ ×•×§×•×¤×™×¨×™×™×˜×™× ×’": "Expert Copywriter. Focus: Psychology, Virality.",
    "×›×ª×™×‘×ª ×§×•×“ ×•×¤×™×ª×•×—": "Software Architect. Focus: Clean Code, Security.",
    "×›×ª×™×‘×” ×™×•×¦×¨×ª": "Storyteller. Focus: Narrative depth.",
    "××¡×˜×¨×˜×’×™×” ×¢×¡×§×™×ª": "Consultant. Focus: Growth, ROI.",
    "×›×œ×œ×™/××—×¨": "Prompt Engineer. Focus: Clarity."
}

MODEL_LINKS = {
    "Claude": "https://claude.ai",
    "GPT": "https://chat.openai.com",
    "Gemini": "https://gemini.google.com"
}

def get_model_link_button(analysis_text):
    target_url = "https://chat.openai.com"
    label = "ChatGPT"
    if "Claude" in analysis_text:
        target_url = MODEL_LINKS["Claude"]
        label = "Claude AI"
    elif "Gemini" in analysis_text:
        target_url = MODEL_LINKS["Gemini"]
        label = "Gemini"
    return target_url, label

def get_api_key():
    try: return st.secrets["GEMINI_API_KEY"]
    except: return ""

def get_working_model():
    try:
        # ×‘×“×™×§×” ××”×™×¨×” ×œ×œ× ×§×¨×™××” ×›×‘×“×” ×œ×¨×©×ª
        return 'gemini-1.5-flash'
    except:
        return 'gemini-pro'

def clean_response(text):
    return text.replace("undefined", "").replace("null", "").strip()

def generate_smart_prompt(api_key, raw_input, context_key, tone):
    try:
        genai.configure(api_key=api_key.strip())
        model_name = get_working_model()
        model = genai.GenerativeModel(model_name)
        
        specific_logic = CONTEXT_LOGIC.get(context_key, CONTEXT_LOGIC["×›×œ×œ×™/××—×¨"])

        full_query = f"""
        Act as a world-class Meta-Prompting System (CO-STAR framework).
        INPUT: Request="{raw_input}", Persona="{specific_logic}", Tone="{tone}".
        TASK:
        1. Write an expert prompt in Hebrew.
        2. Recommend best AI model (Claude/GPT/Gemini).
        OUTPUT FORMAT:
        ---DIVIDER---
        [Hebrew Prompt]
        ---DIVIDER---
        [Recommendation]
        """
        
        response = model.generate_content(full_query)
        return clean_response(response.text), model_name
    except Exception as e:
        if "429" in str(e): return "QUOTA_ERROR", ""
        return f"Error: {str(e)}", ""

# ==========================================
# 5. ×××©×§ ××©×ª××©
# ==========================================
saved_key = get_api_key()

with st.sidebar:
    st.title("âš™ï¸ ×”×’×“×¨×•×ª")
    if saved_key:
        st.success("××¤×ª×— ××—×•×‘×¨ âœ…")
        api_key = saved_key
    else:
        api_key = st.text_input("××¤×ª×— API", type="password")
    
    selected_context = st.selectbox("×ª×—×•×:", list(CONTEXT_LOGIC.keys()))
    selected_tone = st.select_slider("×˜×•×Ÿ:", ["×¨×©××™", "×™×©×™×¨", "×™×¦×™×¨×ª×™", "×©×™×•×•×§×™"], value="×¨×©××™")
    
    st.markdown("---")
    if st.session_state.history:
        st.caption("×”×™×¡×˜×•×¨×™×” ××—×¨×•× ×”:")
        for item in st.session_state.history[:3]:
            st.text(f"ğŸ•’ {item['time']}")
            st.code(item['prompt'][:40] + "...", language="markdown")

st.title("Prompt Pro V12 ğŸ§ ")
st.markdown("##### ××—×•×œ×œ ×¤×¨×•××¤×˜×™× ×—×›×")

user_input = st.text_area("××” ×”××©×™××” ×©×œ×š?", height=100, placeholder="×œ××©×œ: ×¤×•×¡×˜ ×œ×™× ×§×“××™×Ÿ ×¢×œ AI...")

if st.button("×¦×•×¨ ×¤×¨×•××¤×˜ ×× ×¦×— ğŸš€"):
    if not api_key or not user_input:
        st.error("×—×¡×¨ ××¤×ª×— ××• ×˜×§×¡×˜")
    else:
        # --- ××™× ×“×™×§×¦×™×” ×•×™×–×•××œ×™×ª ×œ×—×©×™×‘×” ---
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        status_text.text("ğŸ”„ ××ª×—×‘×¨ ×œ××•×— ×”××œ××›×•×ª×™...")
        time.sleep(0.5) # ×”×©×”×™×™×” ×§×˜× ×” ×œ××¤×§×˜
        progress_bar.progress(30)
        
        status_text.text("âš¡ ×× ×ª×— ××ª ×”×‘×§×©×” ×•×‘×•× ×” ××¡×˜×¨×˜×’×™×”...")
        
        # ×‘×™×¦×•×¢ ×”×¤×¢×•×œ×” ×”×××™×ª×™×ª
        result, used_model = generate_smart_prompt(api_key, user_input, selected_context, selected_tone)
        
        progress_bar.progress(80)
        status_text.text("ğŸ“ ×× ×¡×— ××ª ×”×¤×¨×•××¤×˜ ×”×¡×•×¤×™...")
        time.sleep(0.3)
        
        progress_bar.progress(100)
        time.sleep(0.2)
        progress_bar.empty() # × ×™×§×•×™ ×”×¤×¡ ×‘×¡×™×•×
        status_text.empty()
        # ----------------------------------

        if result == "QUOTA_ERROR":
            st.warning("âš ï¸ ×¢×•××¡ ×¨×’×¢×™ ×¢×œ ×”××•×“×œ. ×× × × ×¡×” ×©×•×‘ ×‘×¢×•×“ ×“×§×”.")
        elif "Error" in result:
            st.error(f"×©×’×™××”: {result}")
        else:
            parts = result.split("---DIVIDER---")
            prompt_content = parts[1] if len(parts) > 1 else result
            analysis_content = parts[2] if len(parts) > 2 else "××™×Ÿ ×”××œ×¦×”."
            
            add_to_history(user_input, prompt_content, analysis_content, used_model)
            
            st.success(f"×”×¤×¨×•××¤×˜ ××•×›×Ÿ! (××•×“×œ: {used_model})")
            st.code(prompt_content.strip(), language="markdown")
            
            url, label = get_model_link_button(analysis_content)
            st.link_button(f"ğŸš€ ×¤×ª×— ×‘-{label}", url, use_container_width=True)
